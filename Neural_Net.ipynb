{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4e96d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Google Drive - Columbia University\\AI4Finance\\Imitation Learning\\FinRL_Imitation_Learning\\Data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "Project = os.path.abspath(os.path.join(os.getcwd(), 'Data')) + '/' # Get the parent directory\n",
    "print(Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4f96211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.optimize as sco\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import PanelOLS, PooledOLS, FamaMacBeth\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import seaborn as sns; sns.set()\n",
    "from pandas.tseries.offsets import *\n",
    "from dateutil.relativedelta import *\n",
    "import datetime as dt\n",
    "import os\n",
    "from IPython.core.pylabtools import figsize\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02e2db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "idx = pd.IndexSlice\n",
    "figsize(18,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "313946a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "      <th>autocorr</th>\n",
       "      <th>buyminusselltrades</th>\n",
       "      <th>buyminusselltrades_abs</th>\n",
       "      <th>buyminussellvol</th>\n",
       "      <th>buyminussellvol_abs</th>\n",
       "      <th>effectivespread_percent_dw</th>\n",
       "      <th>hindex</th>\n",
       "      <th>overnightproduct</th>\n",
       "      <th>percentpriceimpact_lr_dw</th>\n",
       "      <th>percentrealizedspread_lr_dw</th>\n",
       "      <th>priceimpactintraday</th>\n",
       "      <th>quotedspread_percent_tw</th>\n",
       "      <th>realizedvol</th>\n",
       "      <th>realizedvolintraday</th>\n",
       "      <th>askhi</th>\n",
       "      <th>baspread</th>\n",
       "      <th>bidlo</th>\n",
       "      <th>hlspread</th>\n",
       "      <th>ivol_q</th>\n",
       "      <th>ivol_t</th>\n",
       "      <th>me</th>\n",
       "      <th>n30_pos</th>\n",
       "      <th>n5_pos</th>\n",
       "      <th>prc</th>\n",
       "      <th>ret</th>\n",
       "      <th>ret_abs</th>\n",
       "      <th>ret_am</th>\n",
       "      <th>ret_cto</th>\n",
       "      <th>ret_cto_abs</th>\n",
       "      <th>ret_otc</th>\n",
       "      <th>ret_otc_abs</th>\n",
       "      <th>ret_pm</th>\n",
       "      <th>total_n_trades_a</th>\n",
       "      <th>total_n_trades_m</th>\n",
       "      <th>turnover</th>\n",
       "      <th>turnover_am</th>\n",
       "      <th>var_ratio1</th>\n",
       "      <th>var_ratio2</th>\n",
       "      <th>turnover_pm</th>\n",
       "      <th>buyminussellvwap</th>\n",
       "      <th>priceimpactovernight</th>\n",
       "      <th>tsignsqrtdvol1</th>\n",
       "      <th>tsignsqrtdvol2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14663301</th>\n",
       "      <td>93436</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>1.225</td>\n",
       "      <td>2.070</td>\n",
       "      <td>1.670</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>8.105</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>8.318</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>8.961</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.501</td>\n",
       "      <td>8.242</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>6.323</td>\n",
       "      <td>4.113</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-2.146</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14663302</th>\n",
       "      <td>93436</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>2.289</td>\n",
       "      <td>3.012</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>8.156</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>8.109</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>8.948</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.501</td>\n",
       "      <td>7.982</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>6.614</td>\n",
       "      <td>4.140</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>-1.525</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>3.539</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14663303</th>\n",
       "      <td>93436</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>0.719</td>\n",
       "      <td>2.652</td>\n",
       "      <td>3.339</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>7.962</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>8.034</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>8.969</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.501</td>\n",
       "      <td>7.981</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>6.441</td>\n",
       "      <td>3.941</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-2.139</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.028</td>\n",
       "      <td>4.094</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14663304</th>\n",
       "      <td>93436</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>0.459</td>\n",
       "      <td>2.374</td>\n",
       "      <td>3.003</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>7.992</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>8.073</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>8.925</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.501</td>\n",
       "      <td>8.000</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.700</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>4.328</td>\n",
       "      <td>3.897</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-1.969</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>6.079</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14663305</th>\n",
       "      <td>93436</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>7.933</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>8.058</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>8.955</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.501</td>\n",
       "      <td>7.957</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>3.303</td>\n",
       "      <td>3.220</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-1.224</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>4.606</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          permno       date  autocorr  buyminusselltrades  \\\n",
       "14663301   93436 2021-12-27     1.225               2.070   \n",
       "14663302   93436 2021-12-28    -1.065               2.289   \n",
       "14663303   93436 2021-12-29     0.719               2.652   \n",
       "14663304   93436 2021-12-30     0.459               2.374   \n",
       "14663305   93436 2021-12-31    -0.255              -1.380   \n",
       "\n",
       "          buyminusselltrades_abs  buyminussellvol  buyminussellvol_abs  \\\n",
       "14663301                   1.670            0.030               -0.429   \n",
       "14663302                   3.012            0.263               -0.336   \n",
       "14663303                   3.339           -0.058               -0.348   \n",
       "14663304                   3.003            0.155               -0.435   \n",
       "14663305                   0.833            0.127               -0.461   \n",
       "\n",
       "          effectivespread_percent_dw  hindex  overnightproduct  \\\n",
       "14663301                      -0.569  -0.489             0.185   \n",
       "14663302                      -0.570  -0.451             0.088   \n",
       "14663303                      -0.577  -0.594             0.084   \n",
       "14663304                      -0.578  -0.464             0.248   \n",
       "14663305                      -0.576  -0.579             0.076   \n",
       "\n",
       "          percentpriceimpact_lr_dw  percentrealizedspread_lr_dw  \\\n",
       "14663301                    -0.517                       -0.344   \n",
       "14663302                    -0.529                       -0.336   \n",
       "14663303                    -0.526                       -0.347   \n",
       "14663304                    -0.532                       -0.344   \n",
       "14663305                    -0.536                       -0.338   \n",
       "\n",
       "          priceimpactintraday  quotedspread_percent_tw  realizedvol  \\\n",
       "14663301               -0.178                   -0.604       -0.391   \n",
       "14663302               -0.178                   -0.602       -0.531   \n",
       "14663303               -0.178                   -0.601       -0.705   \n",
       "14663304               -0.178                   -0.603       -0.490   \n",
       "14663305               -0.178                   -0.606       -0.855   \n",
       "\n",
       "          realizedvolintraday  askhi  baspread  bidlo  hlspread  ivol_q  \\\n",
       "14663301               -0.491  8.105    -0.475  8.318    -0.328  -0.244   \n",
       "14663302               -0.503  8.156    -0.445  8.109    -0.430  -0.244   \n",
       "14663303               -0.668  7.962    -0.458  8.034    -0.441  -0.244   \n",
       "14663304               -0.570  7.992    -0.474  8.073    -0.382  -0.244   \n",
       "14663305               -0.845  7.933    -0.455  8.058    -0.659  -0.244   \n",
       "\n",
       "          ivol_t    me  n30_pos  n5_pos   prc    ret  ret_abs  ret_am  \\\n",
       "14663301  -0.250 8.961    0.275   0.501 8.242  0.516   -0.166   0.689   \n",
       "14663302  -0.252 8.948    0.275   0.501 7.982 -0.130   -0.717  -0.219   \n",
       "14663303  -0.252 8.969    0.275   0.501 7.981 -0.068   -0.796  -0.491   \n",
       "14663304  -0.252 8.925    0.275   0.501 8.000 -0.335   -0.456   0.700   \n",
       "14663305  -0.252 8.955    0.275   0.501 7.957 -0.294   -0.509  -0.105   \n",
       "\n",
       "          ret_cto  ret_cto_abs  ret_otc  ret_otc_abs  ret_pm  \\\n",
       "14663301    0.682        0.066    0.082       -0.762  -0.660   \n",
       "14663302    0.245       -0.476   -0.336       -0.428  -0.085   \n",
       "14663303   -0.018       -0.802   -0.091       -0.740  -0.103   \n",
       "14663304   -0.649       -0.073    0.121       -0.713  -0.676   \n",
       "14663305    0.059       -0.706   -0.404       -0.341  -0.400   \n",
       "\n",
       "          total_n_trades_a  total_n_trades_m  turnover  turnover_am  \\\n",
       "14663301             6.323             4.113     0.320        0.505   \n",
       "14663302             6.614             4.140     0.193        0.359   \n",
       "14663303             6.441             3.941     0.147        0.248   \n",
       "14663304             4.328             3.897     0.039        0.155   \n",
       "14663305             3.303             3.220    -0.046       -0.007   \n",
       "\n",
       "          var_ratio1  var_ratio2  turnover_pm  buyminussellvwap  \\\n",
       "14663301      -2.146      -1.324        0.085            -0.772   \n",
       "14663302      -1.382      -1.525       -0.010             3.539   \n",
       "14663303      -2.139      -0.425        0.028             4.094   \n",
       "14663304      -1.969      -0.738       -0.083             6.079   \n",
       "14663305      -1.224      -0.758       -0.098             4.606   \n",
       "\n",
       "          priceimpactovernight  tsignsqrtdvol1  tsignsqrtdvol2  \n",
       "14663301                -0.119          -0.286          -0.311  \n",
       "14663302                -0.119          -0.284          -0.310  \n",
       "14663303                -0.119          -0.288          -0.313  \n",
       "14663304                -0.119          -0.288          -0.313  \n",
       "14663305                -0.119          -0.292          -0.317  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 43 daily trading indicators\n",
    "indicators = pd.read_sas(Project+'trading_indicators.sas7bdat')\n",
    "indicators.columns = indicators.columns.str.lower()\n",
    "indicators['permno'] = indicators['permno'].astype(int)\n",
    "indicators.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1f1aab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['autocorr', 'buyminusselltrades', 'buyminusselltrades_abs',\n",
       "       'buyminussellvol', 'buyminussellvol_abs', 'effectivespread_percent_dw',\n",
       "       'hindex', 'overnightproduct', 'percentpriceimpact_lr_dw',\n",
       "       'percentrealizedspread_lr_dw', 'priceimpactintraday',\n",
       "       'quotedspread_percent_tw', 'realizedvol', 'realizedvolintraday',\n",
       "       'askhi', 'baspread', 'bidlo', 'hlspread', 'ivol_q', 'ivol_t', 'me',\n",
       "       'n30_pos', 'n5_pos', 'prc', 'ret', 'ret_abs', 'ret_am', 'ret_cto',\n",
       "       'ret_cto_abs', 'ret_otc', 'ret_otc_abs', 'ret_pm', 'total_n_trades_a',\n",
       "       'total_n_trades_m', 'turnover', 'turnover_am', 'var_ratio1',\n",
       "       'var_ratio2', 'turnover_pm', 'buyminussellvwap', 'priceimpactovernight',\n",
       "       'tsignsqrtdvol1', 'tsignsqrtdvol2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = indicators.columns.drop(['permno','date'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11dcc583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyname</th>\n",
       "      <th>date</th>\n",
       "      <th>russellgroup</th>\n",
       "      <th>sector</th>\n",
       "      <th>price</th>\n",
       "      <th>marketcap</th>\n",
       "      <th>moribvol</th>\n",
       "      <th>moribvol_pctl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12066442</th>\n",
       "      <td>93436</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>1.000</td>\n",
       "      <td>XLY</td>\n",
       "      <td>123.150</td>\n",
       "      <td>388,877.164</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066443</th>\n",
       "      <td>93436</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>1.000</td>\n",
       "      <td>XLY</td>\n",
       "      <td>109.100</td>\n",
       "      <td>344,510.738</td>\n",
       "      <td>0.022</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066444</th>\n",
       "      <td>93436</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>1.000</td>\n",
       "      <td>XLY</td>\n",
       "      <td>112.710</td>\n",
       "      <td>355,910.225</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066445</th>\n",
       "      <td>93436</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>1.000</td>\n",
       "      <td>XLY</td>\n",
       "      <td>121.820</td>\n",
       "      <td>384,677.348</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066446</th>\n",
       "      <td>93436</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1.000</td>\n",
       "      <td>XLY</td>\n",
       "      <td>123.180</td>\n",
       "      <td>388,971.892</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          permno ticker companyname       date  russellgroup sector   price  \\\n",
       "12066442   93436   TSLA   TESLA INC 2022-12-23         1.000    XLY 123.150   \n",
       "12066443   93436   TSLA   TESLA INC 2022-12-27         1.000    XLY 109.100   \n",
       "12066444   93436   TSLA   TESLA INC 2022-12-28         1.000    XLY 112.710   \n",
       "12066445   93436   TSLA   TESLA INC 2022-12-29         1.000    XLY 121.820   \n",
       "12066446   93436   TSLA   TESLA INC 2022-12-30         1.000    XLY 123.180   \n",
       "\n",
       "           marketcap  moribvol  moribvol_pctl  \n",
       "12066442 388,877.164    -0.111             38  \n",
       "12066443 344,510.738     0.022             65  \n",
       "12066444 355,910.225    -0.092             48  \n",
       "12066445 384,677.348    -0.043             57  \n",
       "12066446 388,971.892    -0.025             53  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retail market order imbalance, moribvol_pctl is the 100 groups by sorting on moribvol at each date\n",
    "retail = pd.read_sas(Project+'retail_market_order.sas7bdat', encoding = 'UTF-8')\n",
    "retail.columns = retail.columns.str.lower()\n",
    "retail[['permno','moribvol_pctl']] = retail[['permno','moribvol_pctl']].astype(int)\n",
    "retail = retail[retail['date'] >= '2010-01-01'].reset_index(drop = True)\n",
    "retail.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8287eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top (bottom) 5 percentile as strong buy (sell) by the retail investors\n",
    "retail['Buy'] = np.where(retail['moribvol_pctl'] >= 95, 1, 0)\n",
    "retail['Sell'] = np.where(retail['moribvol_pctl'] <= 5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93d62e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>event_date</th>\n",
       "      <th>date</th>\n",
       "      <th>Buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2010-02-23</td>\n",
       "      <td>2010-02-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2010-02-23</td>\n",
       "      <td>2010-02-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2010-02-23</td>\n",
       "      <td>2010-02-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2010-02-25</td>\n",
       "      <td>2010-02-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2010-02-25</td>\n",
       "      <td>2010-02-25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853659</th>\n",
       "      <td>93435</td>\n",
       "      <td>2010-11-08</td>\n",
       "      <td>2010-11-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853660</th>\n",
       "      <td>93435</td>\n",
       "      <td>2011-03-29</td>\n",
       "      <td>2011-03-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853661</th>\n",
       "      <td>93435</td>\n",
       "      <td>2011-03-29</td>\n",
       "      <td>2011-03-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853662</th>\n",
       "      <td>93435</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853663</th>\n",
       "      <td>93435</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1853664 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         permno event_date       date  Buy\n",
       "0         10001 2010-02-23 2010-02-22    0\n",
       "1         10001 2010-02-23 2010-02-23    1\n",
       "2         10001 2010-02-23 2010-02-24    0\n",
       "3         10001 2010-02-25 2010-02-24    0\n",
       "4         10001 2010-02-25 2010-02-25    1\n",
       "...         ...        ...        ...  ...\n",
       "1853659   93435 2010-11-08 2010-11-09    0\n",
       "1853660   93435 2011-03-29 2011-03-28    0\n",
       "1853661   93435 2011-03-29 2011-03-29    1\n",
       "1853662   93435 2011-03-30 2011-03-30    1\n",
       "1853663   93435 2011-03-30 2011-03-31    0\n",
       "\n",
       "[1853664 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forming training-testing dataset: pick up buy (sell) stock-day, and one day surrounding it without Buy\n",
    "buy = retail[retail['Buy'] == 1][['permno','date','Buy']]\n",
    "buy['event_date'] = buy['date']\n",
    "\n",
    "nobuy1 = retail[retail['Buy'] == 0][['permno','date','Buy']]\n",
    "nobuy1['event_date'] = nobuy1['date']+pd.offsets.BusinessDay(n=1)\n",
    "nobuy1 = pd.merge(nobuy1, buy[['permno','event_date']], on = ['permno','event_date'], how = 'inner')\n",
    "\n",
    "nobuy2 = retail[retail['Buy'] == 0][['permno','date','Buy']]\n",
    "nobuy2['event_date'] = nobuy2['date']+pd.offsets.BusinessDay(n=-1)\n",
    "nobuy2 = pd.merge(nobuy2, buy[['permno','event_date']], on = ['permno','event_date'], how = 'inner')\n",
    "\n",
    "buy = pd.concat([buy, nobuy1, nobuy2], ignore_index = True).set_index(['permno','event_date','date'])\\\n",
    "      .sort_index().reset_index()\n",
    "buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "721a5783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>event_date</th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>autocorr</th>\n",
       "      <th>buyminusselltrades</th>\n",
       "      <th>buyminusselltrades_abs</th>\n",
       "      <th>buyminussellvol</th>\n",
       "      <th>buyminussellvol_abs</th>\n",
       "      <th>effectivespread_percent_dw</th>\n",
       "      <th>hindex</th>\n",
       "      <th>overnightproduct</th>\n",
       "      <th>percentpriceimpact_lr_dw</th>\n",
       "      <th>percentrealizedspread_lr_dw</th>\n",
       "      <th>priceimpactintraday</th>\n",
       "      <th>quotedspread_percent_tw</th>\n",
       "      <th>realizedvol</th>\n",
       "      <th>realizedvolintraday</th>\n",
       "      <th>askhi</th>\n",
       "      <th>baspread</th>\n",
       "      <th>bidlo</th>\n",
       "      <th>hlspread</th>\n",
       "      <th>ivol_q</th>\n",
       "      <th>ivol_t</th>\n",
       "      <th>me</th>\n",
       "      <th>n30_pos</th>\n",
       "      <th>n5_pos</th>\n",
       "      <th>prc</th>\n",
       "      <th>ret</th>\n",
       "      <th>ret_abs</th>\n",
       "      <th>ret_am</th>\n",
       "      <th>ret_cto</th>\n",
       "      <th>ret_cto_abs</th>\n",
       "      <th>ret_otc</th>\n",
       "      <th>ret_otc_abs</th>\n",
       "      <th>ret_pm</th>\n",
       "      <th>total_n_trades_a</th>\n",
       "      <th>total_n_trades_m</th>\n",
       "      <th>turnover</th>\n",
       "      <th>turnover_am</th>\n",
       "      <th>var_ratio1</th>\n",
       "      <th>var_ratio2</th>\n",
       "      <th>turnover_pm</th>\n",
       "      <th>buyminussellvwap</th>\n",
       "      <th>priceimpactovernight</th>\n",
       "      <th>tsignsqrtdvol1</th>\n",
       "      <th>tsignsqrtdvol2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>831648</th>\n",
       "      <td>93435</td>\n",
       "      <td>2010-07-16</td>\n",
       "      <td>2010-07-19</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-1.356</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>1.173</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>1.590</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.987</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.049</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.934</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831649</th>\n",
       "      <td>93435</td>\n",
       "      <td>2010-07-29</td>\n",
       "      <td>2010-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.147</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831650</th>\n",
       "      <td>93435</td>\n",
       "      <td>2010-10-08</td>\n",
       "      <td>2010-10-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831651</th>\n",
       "      <td>93435</td>\n",
       "      <td>2010-11-08</td>\n",
       "      <td>2010-11-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.284</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.431</td>\n",
       "      <td>1.142</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>1.173</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831652</th>\n",
       "      <td>93435</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.663</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>1.641</td>\n",
       "      <td>1.191</td>\n",
       "      <td>-1.371</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.879</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>0.719</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno event_date       date  ind  autocorr  buyminusselltrades  \\\n",
       "831648   93435 2010-07-16 2010-07-19    0    -1.002               0.034   \n",
       "831649   93435 2010-07-29 2010-07-30    0     0.421               0.088   \n",
       "831650   93435 2010-10-08 2010-10-07    0    -0.330               0.026   \n",
       "831651   93435 2010-11-08 2010-11-08    1     0.195               0.180   \n",
       "831652   93435 2011-03-30 2011-03-30    1    -0.850              -0.006   \n",
       "\n",
       "        buyminusselltrades_abs  buyminussellvol  buyminussellvol_abs  \\\n",
       "831648                  -0.455            0.121               -0.587   \n",
       "831649                  -0.401            0.485               -0.198   \n",
       "831650                  -0.463            0.150               -0.555   \n",
       "831651                  -0.309            1.876                1.284   \n",
       "831652                  -0.475           -0.070               -0.508   \n",
       "\n",
       "        effectivespread_percent_dw  hindex  overnightproduct  \\\n",
       "831648                       0.382   0.162             0.088   \n",
       "831649                      -0.208  -0.315             0.088   \n",
       "831650                      -0.097  -0.277             0.088   \n",
       "831651                      -0.269  -0.434             0.088   \n",
       "831652                       0.036   0.296             0.346   \n",
       "\n",
       "        percentpriceimpact_lr_dw  percentrealizedspread_lr_dw  \\\n",
       "831648                    -0.142                        0.469   \n",
       "831649                    -0.134                       -0.152   \n",
       "831650                     0.122                       -0.183   \n",
       "831651                     0.008                       -0.297   \n",
       "831652                    -0.090                        0.088   \n",
       "\n",
       "        priceimpactintraday  quotedspread_percent_tw  realizedvol  \\\n",
       "831648               -0.131                    0.412        0.175   \n",
       "831649               -0.179                   -0.260       -0.509   \n",
       "831650               -0.153                   -0.153       -0.137   \n",
       "831651               -0.188                   -0.206        0.000   \n",
       "831652               -0.242                   -0.187        0.609   \n",
       "\n",
       "        realizedvolintraday  askhi  baspread  bidlo  hlspread  ivol_q  ivol_t  \\\n",
       "831648                0.300 -0.556     0.173 -0.557     0.005  -0.024  -0.128   \n",
       "831649               -0.469 -0.587    -0.337 -0.577    -0.503   0.081  -0.198   \n",
       "831650               -0.089 -0.522    -0.191 -0.520    -0.133  -0.176  -0.137   \n",
       "831651                0.106 -0.396    -0.330 -0.394    -0.265  -0.174  -0.207   \n",
       "831652                0.485 -0.663    -0.294 -0.664     0.383  -0.033  -0.162   \n",
       "\n",
       "           me  n30_pos  n5_pos    prc    ret  ret_abs  ret_am  ret_cto  \\\n",
       "831648 -0.318   -0.847  -1.356 -0.545  1.173    0.683   0.886    0.014   \n",
       "831649 -0.319   -0.011  -0.909 -0.580 -0.343   -0.472  -0.599   -0.180   \n",
       "831650 -0.317    0.268  -1.081 -0.527  0.261   -0.472   0.535    0.404   \n",
       "831651 -0.310    0.547  -0.427 -0.382  0.975    0.431   1.142   -0.041   \n",
       "831652 -0.335    0.198  -0.695 -0.671 -0.267   -0.624  -1.574    1.641   \n",
       "\n",
       "        ret_cto_abs  ret_otc  ret_otc_abs  ret_pm  total_n_trades_a  \\\n",
       "831648       -0.682    1.590        1.158   1.987            -0.345   \n",
       "831649       -0.584   -0.574       -0.125  -0.048            -0.345   \n",
       "831650       -0.227   -0.018       -0.803  -0.311            -0.345   \n",
       "831651       -0.746    1.173        0.649   0.254            -0.325   \n",
       "831652        1.191   -1.371        0.835  -0.008            -0.336   \n",
       "\n",
       "        total_n_trades_m  turnover  turnover_am  var_ratio1  var_ratio2  \\\n",
       "831648            -0.428    -0.662       -0.659       1.073       1.049   \n",
       "831649            -0.423    -0.588       -0.498       1.009       1.147   \n",
       "831650            -0.426    -0.600       -0.471       0.606       0.206   \n",
       "831651            -0.406    -0.202        0.088       0.735       0.670   \n",
       "831652            -0.434    -0.380       -0.237       0.891       0.879   \n",
       "\n",
       "        turnover_pm  buyminussellvwap  priceimpactovernight  tsignsqrtdvol1  \\\n",
       "831648       -0.654             0.934                -0.114           0.400   \n",
       "831649       -0.749            -0.116                -0.151          -0.463   \n",
       "831650       -0.788            -0.427                 0.115           0.213   \n",
       "831651       -0.623             0.397                -0.170           0.296   \n",
       "831652       -0.626             0.719                -0.214          -0.101   \n",
       "\n",
       "        tsignsqrtdvol2  \n",
       "831648           0.414  \n",
       "831649          -0.555  \n",
       "831650           0.208  \n",
       "831651           0.248  \n",
       "831652          -0.031  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.merge(buy.rename(columns = {'Buy':'ind'}), indicators.dropna(subset = features), \n",
    "                  on = ['permno','date'], how = 'inner')\n",
    "sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa81de",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "076e2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5a12913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data: \n",
    "                X = StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.X = torch.tensor(X).float()\n",
    "            self.y = torch.tensor(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return F.softmax(self.l3(a))\n",
    "    \n",
    "    \n",
    "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, labels.type(torch.LongTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs,labels = inputs.to(device),labels.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels.type(torch.LongTensor))\n",
    "            valid_loss+=loss.item()\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7609b1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "seed = 99\n",
    "event_dates = sorted(sample['event_date'].unique())\n",
    "\n",
    "len(event_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e9d3d702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_periods = event_dates[2100:]\n",
    "len(test_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dfb3a3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_periods = event_dates[:2100]\n",
    "len(train_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a0227f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = sample[sample['event_date'].isin(test_periods)]\n",
    "X_test = data_test[features].astype(np.float32).values\n",
    "y_test = data_test['ind'].astype(np.float32).values\n",
    "\n",
    "data_train = sample[sample['event_date'].isin(train_periods)]\n",
    "X_train = data_train[features].astype(np.float32).values\n",
    "y_train = data_train['ind'].astype(np.float32).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "90f0f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "train_dataset = Dataset(X_train, y_train, scale_data=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
    "trade_dataset = Dataset(scaler.transform(X_test), y_test, scale_data=False)\n",
    "\n",
    "# for CV\n",
    "dataset = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1a15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6133d5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dfb6bc9730>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs=10\n",
    "batch_size=512\n",
    "k=5\n",
    "\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "be0e4c51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=43, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/10 Training Loss:500.421 Test Loss:125.419 AVG Training Acc 62.76 % AVG Test Acc 62.54 %\n",
      "Epoch:2/10 Training Loss:498.013 Test Loss:125.255 AVG Training Acc 62.97 % AVG Test Acc 62.60 %\n",
      "Epoch:3/10 Training Loss:497.020 Test Loss:125.252 AVG Training Acc 63.04 % AVG Test Acc 62.63 %\n",
      "Epoch:4/10 Training Loss:496.419 Test Loss:125.076 AVG Training Acc 63.11 % AVG Test Acc 62.72 %\n",
      "Epoch:5/10 Training Loss:495.919 Test Loss:125.071 AVG Training Acc 63.20 % AVG Test Acc 62.70 %\n",
      "Epoch:6/10 Training Loss:495.389 Test Loss:124.908 AVG Training Acc 63.27 % AVG Test Acc 62.72 %\n",
      "Epoch:7/10 Training Loss:494.966 Test Loss:124.979 AVG Training Acc 63.34 % AVG Test Acc 62.62 %\n",
      "Epoch:8/10 Training Loss:494.473 Test Loss:124.881 AVG Training Acc 63.44 % AVG Test Acc 62.73 %\n",
      "Epoch:9/10 Training Loss:494.024 Test Loss:124.988 AVG Training Acc 63.49 % AVG Test Acc 62.68 %\n",
      "Epoch:10/10 Training Loss:493.447 Test Loss:125.025 AVG Training Acc 63.60 % AVG Test Acc 62.71 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=43, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=43, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=43, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=43, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    model = NeuralNet(state_dim=43, action_dim=2, hidden_dim=256)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    if fold != 0: continue\n",
    "        \n",
    "    print('Fold {}'.format(fold + 1))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "        test_loss, test_correct=valid_epoch(model,device,test_loader,criterion)\n",
    "\n",
    "        train_loss = train_loss\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\"Epoch:{}/{} Training Loss:{:.3f} Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             test_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             test_acc))\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134c615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b24cb2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.004\n",
      "Model:                            OLS   Adj. R-squared:                  0.004\n",
      "Method:                 Least Squares   F-statistic:                     1257.\n",
      "Date:                Mon, 22 May 2023   Prob (F-statistic):          7.76e-275\n",
      "Time:                        23:18:05   Log-Likelihood:            -2.3825e+05\n",
      "No. Observations:              342100   AIC:                         4.765e+05\n",
      "Df Residuals:                  342098   BIC:                         4.765e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3554      0.001    305.219      0.000       0.353       0.358\n",
      "x1             0.1130      0.003     35.455      0.000       0.107       0.119\n",
      "==============================================================================\n",
      "Omnibus:                  1394159.161   Durbin-Watson:                   2.614\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            56943.549\n",
      "Skew:                           0.473   Prob(JB):                         0.00\n",
      "Kurtosis:                       1.240   Cond. No.                         4.11\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(trade_dataset.X).detach().numpy()\n",
    "res = sm.OLS(y_test, sm.add_constant(y_pred[:,1])).fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e604452a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 741.979\n",
      "Epoch: 2, loss: 738.892\n",
      "Epoch: 3, loss: 737.875\n",
      "Epoch: 4, loss: 737.061\n",
      "Epoch: 5, loss: 736.389\n",
      "Epoch: 6, loss: 735.766\n",
      "Epoch: 7, loss: 735.180\n",
      "Epoch: 8, loss: 734.588\n",
      "Epoch: 9, loss: 734.041\n",
      "Epoch: 10, loss: 733.483\n",
      "Epoch: 11, loss: 732.938\n",
      "Epoch: 12, loss: 732.371\n",
      "Epoch: 13, loss: 731.799\n",
      "Epoch: 14, loss: 731.213\n",
      "Epoch: 15, loss: 730.620\n",
      "Epoch: 16, loss: 730.036\n",
      "Epoch: 17, loss: 729.436\n",
      "Epoch: 18, loss: 728.845\n",
      "Epoch: 19, loss: 728.215\n",
      "Epoch: 20, loss: 727.599\n",
      "Epoch: 21, loss: 726.969\n",
      "Epoch: 22, loss: 726.323\n",
      "Epoch: 23, loss: 725.683\n",
      "Epoch: 24, loss: 725.038\n",
      "Epoch: 25, loss: 724.394\n",
      "Epoch: 26, loss: 723.737\n",
      "Epoch: 27, loss: 723.083\n",
      "Epoch: 28, loss: 722.414\n",
      "Epoch: 29, loss: 721.760\n",
      "Epoch: 30, loss: 721.057\n",
      "Epoch: 31, loss: 720.359\n",
      "Epoch: 32, loss: 719.675\n",
      "Epoch: 33, loss: 718.992\n",
      "Epoch: 34, loss: 718.289\n",
      "Epoch: 35, loss: 717.605\n",
      "Epoch: 36, loss: 716.888\n",
      "Epoch: 37, loss: 716.247\n",
      "Epoch: 38, loss: 715.542\n",
      "Epoch: 39, loss: 714.805\n",
      "Epoch: 40, loss: 714.132\n",
      "Epoch: 41, loss: 713.450\n",
      "Epoch: 42, loss: 712.790\n",
      "Epoch: 43, loss: 712.098\n",
      "Epoch: 44, loss: 711.420\n",
      "Epoch: 45, loss: 710.794\n",
      "Epoch: 46, loss: 710.086\n",
      "Epoch: 47, loss: 709.444\n",
      "Epoch: 48, loss: 708.783\n",
      "Epoch: 49, loss: 708.094\n",
      "Epoch: 50, loss: 707.478\n",
      "Epoch: 51, loss: 706.801\n",
      "Epoch: 52, loss: 706.157\n",
      "Epoch: 53, loss: 705.458\n",
      "Epoch: 54, loss: 704.786\n",
      "Epoch: 55, loss: 704.258\n",
      "Epoch: 56, loss: 703.589\n",
      "Epoch: 57, loss: 702.993\n",
      "Epoch: 58, loss: 702.349\n",
      "Epoch: 59, loss: 701.777\n",
      "Epoch: 60, loss: 701.125\n",
      "Epoch: 61, loss: 700.505\n",
      "Epoch: 62, loss: 699.937\n",
      "Epoch: 63, loss: 699.299\n",
      "Epoch: 64, loss: 698.804\n",
      "Epoch: 65, loss: 698.223\n",
      "Epoch: 66, loss: 697.673\n",
      "Epoch: 67, loss: 696.984\n",
      "Epoch: 68, loss: 696.493\n",
      "Epoch: 69, loss: 696.032\n",
      "Epoch: 70, loss: 695.442\n",
      "Epoch: 71, loss: 694.843\n",
      "Epoch: 72, loss: 694.350\n",
      "Epoch: 73, loss: 693.886\n",
      "Epoch: 74, loss: 693.383\n",
      "Epoch: 75, loss: 692.702\n",
      "Epoch: 76, loss: 692.233\n",
      "Epoch: 77, loss: 691.755\n",
      "Epoch: 78, loss: 691.294\n",
      "Epoch: 79, loss: 690.779\n",
      "Epoch: 80, loss: 690.326\n",
      "Epoch: 81, loss: 689.795\n",
      "Epoch: 82, loss: 689.392\n",
      "Epoch: 83, loss: 688.810\n",
      "Epoch: 84, loss: 688.428\n",
      "Epoch: 85, loss: 688.304\n",
      "Epoch: 86, loss: 687.773\n",
      "Epoch: 87, loss: 687.390\n",
      "Epoch: 88, loss: 686.812\n",
      "Epoch: 89, loss: 686.388\n",
      "Epoch: 90, loss: 685.936\n",
      "Epoch: 91, loss: 685.668\n",
      "Epoch: 92, loss: 684.904\n",
      "Epoch: 93, loss: 684.626\n",
      "Epoch: 94, loss: 684.221\n",
      "Epoch: 95, loss: 683.837\n",
      "Epoch: 96, loss: 683.446\n",
      "Epoch: 97, loss: 683.071\n",
      "Epoch: 98, loss: 682.404\n",
      "Epoch: 99, loss: 681.903\n",
      "Epoch: 100, loss: 681.710\n",
      "Epoch: 101, loss: 681.234\n",
      "Epoch: 102, loss: 681.030\n",
      "Epoch: 103, loss: 680.401\n",
      "Epoch: 104, loss: 680.098\n",
      "Epoch: 105, loss: 679.799\n",
      "Epoch: 106, loss: 679.354\n",
      "Epoch: 107, loss: 679.055\n",
      "Epoch: 108, loss: 678.752\n",
      "Epoch: 109, loss: 678.431\n",
      "Epoch: 110, loss: 677.986\n",
      "Epoch: 111, loss: 677.572\n",
      "Epoch: 112, loss: 677.367\n",
      "Epoch: 113, loss: 676.927\n",
      "Epoch: 114, loss: 676.551\n",
      "Epoch: 115, loss: 676.548\n",
      "Epoch: 116, loss: 675.868\n",
      "Epoch: 117, loss: 675.342\n",
      "Epoch: 118, loss: 675.256\n",
      "Epoch: 119, loss: 674.478\n",
      "Epoch: 120, loss: 674.448\n",
      "Epoch: 121, loss: 674.153\n",
      "Epoch: 122, loss: 674.751\n",
      "Epoch: 123, loss: 673.957\n",
      "Epoch: 124, loss: 673.564\n",
      "Epoch: 125, loss: 673.541\n",
      "Epoch: 126, loss: 672.934\n",
      "Epoch: 127, loss: 672.898\n",
      "Epoch: 128, loss: 672.359\n",
      "Epoch: 129, loss: 672.066\n",
      "Epoch: 130, loss: 671.989\n",
      "Epoch: 131, loss: 671.385\n",
      "Epoch: 132, loss: 671.620\n",
      "Epoch: 133, loss: 671.371\n",
      "Epoch: 134, loss: 671.160\n",
      "Epoch: 135, loss: 670.563\n",
      "Epoch: 136, loss: 670.796\n",
      "Epoch: 137, loss: 670.971\n",
      "Epoch: 138, loss: 672.020\n",
      "Epoch: 139, loss: 670.829\n",
      "Epoch: 140, loss: 670.895\n",
      "Epoch: 141, loss: 671.055\n",
      "Epoch: 142, loss: 670.954\n",
      "Epoch: 143, loss: 670.967\n",
      "Epoch: 144, loss: 670.604\n",
      "Epoch: 145, loss: 670.436\n",
      "Epoch: 146, loss: 669.456\n",
      "Epoch: 147, loss: 669.595\n",
      "Epoch: 148, loss: 669.061\n",
      "Epoch: 149, loss: 669.042\n",
      "Epoch: 150, loss: 667.871\n",
      "Epoch: 151, loss: 667.519\n",
      "Epoch: 152, loss: 667.069\n",
      "Epoch: 153, loss: 668.076\n",
      "Epoch: 154, loss: 667.574\n",
      "Epoch: 155, loss: 667.044\n",
      "Epoch: 156, loss: 666.480\n",
      "Epoch: 157, loss: 666.292\n",
      "Epoch: 158, loss: 666.875\n",
      "Epoch: 159, loss: 666.963\n",
      "Epoch: 160, loss: 667.003\n",
      "Epoch: 161, loss: 666.893\n",
      "Epoch: 162, loss: 666.282\n",
      "Epoch: 163, loss: 666.427\n",
      "Epoch: 164, loss: 665.636\n",
      "Epoch: 165, loss: 665.196\n",
      "Epoch: 166, loss: 664.680\n",
      "Epoch: 167, loss: 665.276\n",
      "Epoch: 168, loss: 664.992\n",
      "Epoch: 169, loss: 664.156\n",
      "Epoch: 170, loss: 664.676\n",
      "Epoch: 171, loss: 664.606\n",
      "Epoch: 172, loss: 664.250\n",
      "Epoch: 173, loss: 663.791\n",
      "Epoch: 174, loss: 663.094\n",
      "Epoch: 175, loss: 662.751\n",
      "Epoch: 176, loss: 663.877\n",
      "Epoch: 177, loss: 663.975\n",
      "Epoch: 178, loss: 664.292\n",
      "Epoch: 179, loss: 665.145\n",
      "Epoch: 180, loss: 668.823\n",
      "Epoch: 181, loss: 668.452\n",
      "Epoch: 182, loss: 667.986\n",
      "Epoch: 183, loss: 665.567\n",
      "Epoch: 184, loss: 664.448\n",
      "Epoch: 185, loss: 665.451\n",
      "Epoch: 186, loss: 664.349\n",
      "Epoch: 187, loss: 664.634\n",
      "Epoch: 188, loss: 664.488\n",
      "Epoch: 189, loss: 665.221\n",
      "Epoch: 190, loss: 664.607\n",
      "Epoch: 191, loss: 664.336\n",
      "Epoch: 192, loss: 664.228\n",
      "Epoch: 193, loss: 664.362\n",
      "Epoch: 194, loss: 665.418\n",
      "Epoch: 195, loss: 665.205\n",
      "Epoch: 196, loss: 663.477\n",
      "Epoch: 197, loss: 663.576\n",
      "Epoch: 198, loss: 664.892\n",
      "Epoch: 199, loss: 663.046\n",
      "Epoch: 200, loss: 662.046\n"
     ]
    }
   ],
   "source": [
    "# net = NeuralNet(state_dim=43, \n",
    "#                 action_dim=2, \n",
    "#                 hidden_dim=256)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\n",
    "# num_epochs = 200\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "#         optimizer.zero_gra d()\n",
    "#         outputs = net(inputs)\n",
    "#         loss = criterion(outputs, targets.type(torch.LongTensor))        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "#     # print statistics\n",
    "#     print(f'Epoch: {epoch + 1}, loss: {running_loss:.3f}')\n",
    "#     running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd01dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl",
   "language": "python",
   "name": "finrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
